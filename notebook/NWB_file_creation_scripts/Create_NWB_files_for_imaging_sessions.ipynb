{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Shih-Yi\\\\Documents\\\\GitHub\\\\swac\\\\+swac_glm')\n",
    "\n",
    "import os\n",
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create virtual modules for swac and swac_beh\n",
    "swac = dj.create_virtual_module('swac','swac')\n",
    "swac_beh = dj.create_virtual_module('swac_beh','swac_beh')\n",
    "swac_glm = dj.create_virtual_module('swac_glm','swac_glm')\n",
    "\n",
    "from pynwb import NWBHDF5IO, NWBFile, TimeSeries, load_namespaces, get_class\n",
    "from pynwb.file import Subject\n",
    "from pynwb.behavior import SpatialSeries, Position, BehavioralTimeSeries\n",
    "from pynwb.ophys import ImageSegmentation, OpticalChannel, ImagingPlane, RoiResponseSeries, DfOverF\n",
    "from pynwb.base import Images\n",
    "from pynwb.image import GrayscaleImage, ImageSeries\n",
    "\n",
    "from ndx_harvey_swac import LabMetaDataSession\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "import h5py\n",
    "from numpy.matlib import repmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31606c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sugery_note = {\n",
    "    3:'cranial window creation date:2017-04-03, 3.5 mm diameter, left posterior cortex; AAVretro injection date:2017-04-03; GCaMP6s injection date:2017-04-03; performed by Shih-Yi Tseng and Selmaan N. Chettih',\n",
    "    4:'cranial window creation date:2017-05-03, 3.5 mm diameter, left posterior cortex; AAVretro injection date:2017-05-03; GCaMP6s injection date:2017-05-03; performed by Shih-Yi Tseng and Selmaan N. Chettih',\n",
    "    5:'cranial window creation date:2017-05-29, 3.5 mm diameter, left posterior cortex; AAVretro injection date:2017-05-29; GCaMP6s injection date:2017-05-29; performed by Shih-Yi Tseng and Selmaan N. Chettih',\n",
    "    6:'cranial window creation date:2017-06-05, 3.5 mm diameter, left posterior cortex; AAVretro injection date:2017-06-05; GCaMP6s injection date:2017-06-12; performed by Shih-Yi Tseng and Selmaan N. Chettih',\n",
    "    7:'cranial window creation date:2017-05-12, 3.5 mm diameter, left posterior cortex; AAVretro injection date:2017-05-12; GCaMP6s injection date:2017-07-05; performed by Shih-Yi Tseng and Selmaan N. Chettih',\n",
    "    8:'cranial window creation date:2017-05-17, 3.5 mm diameter, left posterior cortex; AAVretro injection date:2017-05-17; GCaMP6s injection date:2017-07-12; performed by Shih-Yi Tseng and Selmaan N. Chettih',\n",
    "    9:'cranial window creation date:2017-05-25, 3.5 mm diameter, left posterior cortex; AAVretro injection date:2017-05-25; GCaMP6s injection date:2017-07-26; performed by Shih-Yi Tseng and Selmaan N. Chettih',\n",
    "    10:'cranial window creation date:2017-05-26, 3.5 mm diameter, left posterior cortex; AAVretro injection date:2017-05-26; GCaMP6s injection date:2017-09-11; performed by Shih-Yi Tseng and Selmaan N. Chettih'\n",
    "}\n",
    "\n",
    "virus_note = {\n",
    "    3:'''AAV2/1-synapsin-GCaMP6s-WPRE-SV40 in left V1 x3 sites, PM x1, AM x0, MM x1, RSC x1, visA x1, 1/10 dilution, 140nl per site in L23 and 140nl per site in L5; AAV2retro-Syn-mTagBFP2 in left anterior ACC/M2 x3, undiluted, 300 nl per site, coordinate (mm from bregma): (A 0.95, L 0.6, D 1.0), (A 0.95, L 0.6, D 0.3), (A 0.95, L 0.8, D 0.3); AAV2retro-Syn-mScarlet in left striatum x3, 1/20 dilution, 300 nl per site, coordinate (mm from bregma): (A 0.95, L 1.2, D 2.1), (A 1.0, L 1.75, D 2.1), (P 0.15, L 2.15, D 2.1); mScarlet expression did not come up when examining it under the 2P microscope (due to the higher dilution)''',\n",
    "    4:'''AAV2/1-synapsin-GCaMP6s-WPRE-SV40 in left V1 x3 sites, PM x1, AM x0, MM x1, RSC x0, visA x2, 1/10 dilution, 100nl per site in L23 and 140nl per site in L5; AAV2retro-Syn-mTagBFP2 in left anterior ACC/M2 x3, undiluted, 300 nl per site, coordinate (mm from bregma): (A 0.9, L 0.58, D 0.3), (A 0.9, L 0.58, D 1.0), (A 0.9, L 0.8, D 0.4); AAV2retro-Syn-mScarlet in left striatum x3, 1/5 dilution, 300 nl per site for the anterior sites and 600 nl for the posterior site, coordinate (mm from bregma): (A 1.0, L 1.2, D 2.1), (A 1.0, L 1.5, D 2.1), (P 0.2, L 1.75, D 2.1)''',\n",
    "    5:'''AAV2/1-synapsin-GCaMP6s-WPRE-SV40 in left V1 x3 sites, PM x1, AM x1, MM x1, RSC x0, visA x2, 1/5 dilution, 100nl per site in L23 and 140nl per site in L5; AAV2retro-Syn-mTagBFP2 in left anterior ACC/M2 x3, undiluted, 300 nl per site, coordinate (mm from bregma): (A 0.96, L 0.4, D 0.3), (A 0.96, L 0.4, D 1.0), (A 0.96, L 0.8, D 0.4); AAV2retro-Syn-mScarlet in left striatum x3, 1/5 dilution, 300 nl per site for the anterior sites and 600 nl for the posterior site, coordinate (mm from bregma): (A 0.96, L 1.2, D 2.1), (A 0.96, L 1.5, D 2.1), (P 0.24, L 1.8, D 2.1)''',\n",
    "    6:'''AAV2/1-synapsin-GCaMP6s-WPRE-SV40 in left V1 x3 sites, PM x1, AM x2, MM x1, RSC x1, visA x2, 1/10 dilution, 100nl per site in L23 and 140nl per site in L5; AAV2retro-Syn-mTagBFP2 in left posterior ACC/M2 x4, undiluted, 300 nl per site, coordinate (mm from bregma): (0, L 0.3, D 0.4), (0, L 0.3, D 0.8), (0, L 0.7, D 0.3), (0, L 0.7, D 0.75); AAV2retro-Syn-mScarlet in left ORBvl x1, ORBl x1, 1/5 dilution, 500 nl per site, coordinate (mm from bregma): (A 2.45, L 0.75, D 1.85), (A 2.45, L 1.26, D 1.85)''',\n",
    "    7:'''AAV2/1-synapsin-GCaMP6s-WPRE-SV40 in left V1 x2 sites, PM x1, AM x1, MM x1, RSC x1, visA x2, 1/10 dilution, 70nl per site in L23 and 100nl per site in L5; AAV2retro-Syn-mTagBFP2 in left anterior ACC/M2 x3, undiluted, 300 nl per site, coordinate (mm from bregma): (A 0.7, L 0.5, D 0.3), (A 0.7, L 0.5, D 1.0), (A 1.0, L 0.8, D 0.4); AAV2retro-Syn-mScarlet in left striatum x3, 1/5 dilution, 300 nl per site for the anterior sites and 600 nl for the posterior site, coordinate (mm from bregma): (A 0.8, L 1.2, D 2.1), (A 1.15, L 1.5, D 2.1), (P 0.2, L 1.8, D 2.1)''',\n",
    "    8:'''AAV2/1-synapsin-GCaMP6s-WPRE-SV40 in left V1 x2 sites, PM x1, AM x1, MM x1, RSC x1, visA x2, 1/10 dilution, 70nl per site in L23 and 100nl per site in L5; AAV2retro-Syn-mTagBFP2 in left anterior ACC/M2 x3, undiluted, 300 nl per site, coordinate (mm from bregma): (A 1.1, L 0.55, D 0.3), (A 1.1, L 0.55, D 1.0), (A 0.97, L 0.8, D 0.4); AAV2retro-Syn-mScarlet in left striatum x3, 1/5 dilution, 300 nl per site for the anterior sites and 600 nl for the posterior site, coordinate (mm from bregma): (A 0.98, L 1.2, D 2.1), (A 0.85, L 1.5, D 2.1), (P 0.152, L 1.8, D 2.1)''',\n",
    "    9:'''AAV2/1-synapsin-GCaMP6s-WPRE-SV40 in left V1 x2 sites, PM x1, AM x1, MM x1, RSC x1, visA x2, 1/10 dilution, 70nl per site in L23 and 100nl per site in L5; AAV2retro-Syn-mTagBFP2 in left posterior ACC/M2 x4, undiluted, 300 nl per site, coordinate (mm from bregma): (0, L 0.35, D 0.4), (0, L 0.35, D 0.8), (0, L 0.7, D 0.3), (0, L 0.7, D 0.8); AAV2retro-Syn-mScarlet in left ORBvl x1, ORBl x1, 1/5 dilution, 500 nl per site, coordinate (mm from bregma): (A 2.65, L 0.85, D 1.8), (A 2.6, L 1.35, D 1.75)''',\n",
    "    10:'''AAV2/1-synapsin-GCaMP6s-WPRE-SV40 in left V1 x2 sites, PM x1, AM x1, MM x1, RSC x1, visA x2, 1/10 dilution, 70nl per site in L23 and 100nl per site in L5; AAV2retro-Syn-mTagBFP2 in left posterior ACC/M2 x4, undiluted, 300 nl per site, coordinate (mm from bregma): (A 0.25, L 0.3, D 0.4), (A 0.25, L 0.3, D 0.85), (A 0.1, L 0.7, D 0.3), (A 0.1, L 0.7, D 0.8); AAV2retro-Syn-mScarlet in left  ORBvl x1, ORBl x1, 1/5 dilution, 500 nl per site, coordinate (mm from bregma): (A 2.45, L 0.75, D 1.8), (A 2.45, L 1.25, D 1.85)'''\n",
    "}\n",
    "\n",
    "retro_site = {\n",
    "    3:{'mTagBFP2':'anterior_ACC_M2','mScarlet':'striatum'},\n",
    "    4:{'mTagBFP2':'anterior_ACC_M2','mScarlet':'striatum'},\n",
    "    5:{'mTagBFP2':'anterior_ACC_M2','mScarlet':'striatum'},\n",
    "    6:{'mTagBFP2':'posterior_ACC_M2','mScarlet':'OFC'},\n",
    "    7:{'mTagBFP2':'anterior_ACC_M2','mScarlet':'striatum'},\n",
    "    8:{'mTagBFP2':'anterior_ACC_M2','mScarlet':'striatum'},\n",
    "    9:{'mTagBFP2':'posterior_ACC_M2','mScarlet':'OFC'},\n",
    "    10:{'mTagBFP2':'posterior_ACC_M2','mScarlet':'OFC'}\n",
    "}\n",
    "\n",
    "first_session = {\n",
    "    3:'2017-04-27',\n",
    "    4:'2017-05-13',\n",
    "    5:'2017-06-13',\n",
    "    6:'2017-06-27',\n",
    "    7:'2017-07-14',\n",
    "    8:'2017-07-25',\n",
    "    9:'2017-08-07',\n",
    "    10:'2017-09-21'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b612db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prob_of_action(strat_preds, trial_dict):\n",
    "    '''Compute model probability of action from taskRNN prediction\n",
    "    Input:\n",
    "    strat_preds: policy matrix of taskRNN, in the order of WR, WL, BR, BL\n",
    "    trial_dict: dictionary containing trial type info\n",
    "    \n",
    "    Output:\n",
    "    full_prob: full model probability of the action\n",
    "    cho_prob: marginal choice probability\n",
    "    cue_prob: improvement of action probability given the cue\n",
    "    '''\n",
    "    \n",
    "    # convert cue and choice back to B=1, W=0 & L=1, R=0\n",
    "    cueB = trial_dict['trialCueB'].reshape(-1,)\n",
    "    cueB = (cueB - np.min(cueB))/(np.max(cueB) - np.min(cueB))\n",
    "    \n",
    "    choL = trial_dict['trialChoL'].reshape(-1,)\n",
    "    choL = (choL - np.min(choL))/(np.max(choL) - np.min(choL))\n",
    "    \n",
    "    # scale policy matrix to be *conditioned* on cue\n",
    "    cond_pred = strat_preds * 2\n",
    "    \n",
    "    # calculate marginal choice probability (order of strat_preds: WR, WL, BR, BL)\n",
    "    prob_right = (strat_preds[0,:]+strat_preds[2,:]).reshape(1,-1)\n",
    "    prob_left = (strat_preds[1,:]+strat_preds[3,:]).reshape(1,-1)    \n",
    "    marg_pred = np.concatenate((prob_right,prob_left), axis = 0)\n",
    "    \n",
    "    # get indices for each trials choice\n",
    "    cond_ind = (cueB*2 + choL).astype(int)\n",
    "    marg_ind = choL.astype(int)\n",
    "    \n",
    "    # calculate\n",
    "    full_prob = np.array([cond_pred[cond, ind] for ind, cond in enumerate(cond_ind)])\n",
    "    cho_prob = np.array([marg_pred[marg, ind] for ind, marg in enumerate(marg_ind)])\n",
    "        \n",
    "    # break full conditional prediction into marginal and improvement from marginal (cue_prob)\n",
    "    cue_prob = full_prob - cho_prob\n",
    "    \n",
    "    return full_prob, cho_prob, cue_prob\n",
    "\n",
    "\n",
    "def partition_areas(ml, ap):\n",
    "    from collections import OrderedDict\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from shapely.geometry import Point, Polygon\n",
    "    from scipy.io import loadmat\n",
    "   \n",
    "    # load area partition\n",
    "    area = loadmat('C:\\\\Users\\\\Shih-Yi\\\\Documents\\\\GitHub\\\\swac\\\\AllenMouseCCF\\\\allenAreaPartitions.mat')\n",
    "    areas = ['V1', 'PM', 'AM', 'MM', 'RSC', 'NP']\n",
    "    \n",
    "    # get point objects for sources\n",
    "    coor = np.stack([ml, ap], axis = 1)\n",
    "    points = [Point(coor[i,:]) for i in range(coor.shape[0])]\n",
    "    \n",
    "    # get polygon objects for area\n",
    "    all_poly = {a:Polygon(area[a]) for a in areas}\n",
    "    area_ind = OrderedDict({name:np.array([pt.within(poly) for pt in points]) for name, poly in all_poly.items()})\n",
    "    \n",
    "    binary_id = np.zeros((ml.shape[0], len(areas)))\n",
    "    for i, idx in enumerate(area_ind.values()):\n",
    "        binary_id[:,i] = idx\n",
    "    \n",
    "    all_labels = np.argmax(binary_id, axis = 1)\n",
    "    bad_ids = np.sum(binary_id, axis = 1) != 1\n",
    "    if np.sum(bad_ids)>0:\n",
    "        # for single entry data, use the first True\n",
    "        if ml.shape[0] == 1:\n",
    "            prev_true_exists = False\n",
    "            for idx, (this_area, val) in enumerate(area_ind.items()):\n",
    "                if not prev_true_exists:\n",
    "                    if val[0]: \n",
    "                        prev_true_exists = True\n",
    "                elif prev_true_exists:\n",
    "                    if val[0]:\n",
    "                        area_ind[this_area] = np.array([False])\n",
    "        \n",
    "        # for multiple entry data, find nearest neigbhor\n",
    "        elif ml.shape[0] >= 1:\n",
    "            good_ids = ~bad_ids\n",
    "\n",
    "            good_pts = np.stack((ml[good_ids],ap[good_ids]),axis = 1)\n",
    "            good_labels = all_labels[good_ids]\n",
    "            bad_pts = np.stack((ml[bad_ids],ap[bad_ids]),axis = 1)\n",
    "            NN = NearestNeighbors(n_neighbors=1, radius=0.2).fit(good_pts) \n",
    "            neighbors = NN.kneighbors(bad_pts, return_distance=False) \n",
    "            new_labels = good_labels[neighbors.flatten()]\n",
    "            all_labels[bad_ids] = new_labels\n",
    "\n",
    "            area_ind = OrderedDict({name: all_labels == i for i, name in enumerate(area_ind.keys())})   \n",
    "\n",
    "    \n",
    "    area_ind['RSC'] = area_ind.pop('RSC')\n",
    "    area_ind['A/RL'] = area_ind.pop('NP')\n",
    "        \n",
    "    return area_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5280c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all imaging sessions\n",
    "all_sessions = swac.Session.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72532a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_sess, this_sess in enumerate(all_sessions):\n",
    "    \n",
    "    # delete previous nwbfile\n",
    "    if i_sess > 0:\n",
    "        del nwbfile\n",
    "        \n",
    "        \n",
    "    #### SESSION INFO PREPARATION ####\n",
    "    # process key\n",
    "    key = {'mouse_id': this_sess[0], 'session_date': this_sess[1]}\n",
    "    mouse_id = key['mouse_id']\n",
    "    session_date = key['session_date']\n",
    "    \n",
    "    print(f'Start processing mouse_id = {mouse_id}, session_date = {session_date}')\n",
    "    \n",
    "    # fetch session info\n",
    "    acq_name, layer, img_type, fov, step_size, laser_power, lz, sync_loc, behav_loc, session_note = (swac.SessionInfo & key).fetch1(\n",
    "        'acq_name', 'layer', 'type', 'fov', 'step_size', 'laser_power', 'lz','sync_loc', 'behav_loc','notes')\n",
    "    if session_note=='':\n",
    "        session_note = None\n",
    "\n",
    "    if layer == '2/3':\n",
    "        layer_name = '23'\n",
    "    else:\n",
    "        layer_name = 5\n",
    "        \n",
    "    if img_type == 'plane':\n",
    "        img_type_name = 'single_plane'\n",
    "    elif img_type == 'volume':\n",
    "        img_type_name = 'multi_plane'\n",
    "        \n",
    "    # fetch FOV center location\n",
    "    center_ml, center_ap, depth = (swac.FOVtoCCF & key).fetch1('center_ml_ccf_mm', 'center_ap_ccf_mm', 'depth_mm')\n",
    "\n",
    "    # find area that FOV centered on \n",
    "    area_ind = partition_areas(np.array(center_ml).reshape(-1,1), np.array(center_ap).reshape(-1,1))\n",
    "    fov_center_area = [area_name for idx, area_name in enumerate(area_ind.keys()) if area_ind[area_name][0] == True][0]\n",
    "    if fov_center_area == 'A/RL':\n",
    "        fov_center_area = 'visA'\n",
    "        \n",
    "    # fetch raw behavioral data\n",
    "    expt, switches, frac_no_checker, penalty, delay, performance, init_block, variables, behav_data = (swac.Behavior & key).fetch1(\n",
    "     'expt', 'switches', 'frac_no_checker', 'penalty', 'delay', 'performance','init_block', 'variables', 'behav_data')\n",
    "\n",
    "    if delay is None:\n",
    "        delay = 0.\n",
    "\n",
    "    # extract behavioral data\n",
    "    world = behav_data[0,:]\n",
    "    lat_vel_verm = behav_data[1,:]\n",
    "    for_vel_verm = behav_data[2,:]\n",
    "    lat_pos = behav_data[4,:]\n",
    "    for_pos = behav_data[5,:]\n",
    "    trial_phase = behav_data[7,:]\n",
    "    reward = behav_data[8,:]\n",
    "    trial_num = behav_data[10,:]-1 # set first trial to 0\n",
    "\n",
    "    # extract task variables\n",
    "    var_names = list(variables.dtype.names)\n",
    "    variable_dict = {var_names[ind]:variables[var_names[ind]][0][0][0] for ind, item in enumerate(var_names)}\n",
    "    floorLength = float(variable_dict['floorLength'])\n",
    "    funnelLength = float(variable_dict['funnelLength'])\n",
    "    floorWidth = float(variable_dict['floorWidth'])\n",
    "    funnelWidth = float(variable_dict['funnelWidth'])\n",
    "    wallHeight = float(variable_dict['wallHeight'])\n",
    "    hideCuePast = float(variable_dict['hideCuePast'])\n",
    "    fractionNoChecker = float(variable_dict['fractionNoChecker'])\n",
    "    penaltyProb = float(variable_dict['penaltyProb'])\n",
    "    feedbackDelay = float(variable_dict['feedbackDelay'])\n",
    "    rewardDelay = float(variable_dict['rewardDelay'])\n",
    "    itiCorrect = float(variable_dict['itiCorrect'])\n",
    "    itiMissBase = float(variable_dict['itiMissBase'])\n",
    "    \n",
    "    # generate note for maze configuration\n",
    "    maze_note = f'maze_stem_length={floorLength};maze_stem_width={floorWidth};maze_arm_length={funnelLength};maze_arm_width={funnelWidth};cue_delay_length={delay};wall_height={wallHeight};max_position={hideCuePast};frac_non_visually_guided_trials={frac_no_checker};choice_bias_penalty={penalty};feedback_delay={feedbackDelay};reward_delay={rewardDelay};iti_correct={itiCorrect};iti_incorrect={itiMissBase}'\n",
    "\n",
    "    # read sync file (rows: pitch, roll, yaw, imaging frame, vermin frame, lick signals)\n",
    "    sync_data = h5py.File(sync_loc, 'r')['sweep_0001']['analogScans']\n",
    "    sync_rate = 2000.0\n",
    "    session_duration = sync_data.shape[1]/sync_rate\n",
    "\n",
    "    # get session start time by looking at the modification time of the sync file\n",
    "    m_time = os.path.getmtime(sync_loc)\n",
    "    session_start_time = datetime.fromtimestamp(m_time-session_duration)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### NWB FILE INITIATION ####\n",
    "    # create NWB file\n",
    "    nwbfile = NWBFile(\n",
    "        session_description = f'Mouse performing a dynamic navigation task with calcium imaging in {fov_center_area} layer {layer}',\n",
    "        identifier = f'Mouse_{mouse_id}_session_date_{session_date}',\n",
    "        session_start_time = session_start_time, #datetime.strptime(str(session_date), '%Y-%m-%d'),\n",
    "        session_id = f'mouse_{mouse_id}_session_date_{session_date}_area_{fov_center_area}_L{layer_name}_{img_type_name}_imaging',      \n",
    "        keywords = ['mouse', 'decision-making', 'navigation', 'virtual reality', 'cortex', 'calcium imaging', 'two-photon imaging'],\n",
    "        surgery = sugery_note[mouse_id],\n",
    "        virus = virus_note[mouse_id],\n",
    "        stimulus_notes = maze_note,\n",
    "        notes = session_note,\n",
    "        experimenter = 'Tseng, Shih-Yi',   \n",
    "        experiment_description = f'Mouse performing a dynamic navigation task with calcium imaging in {fov_center_area} layer {layer}',\n",
    "        lab = 'Harvey Lab',                                       \n",
    "        institution = 'Harvard Medical School',               \n",
    "        related_publications = 'doi:10.1016/j.neuron.2022.05.012'\n",
    "    )\n",
    "    \n",
    "    # fetch mouse info\n",
    "    mouse_name, doa, training_start = (swac.Mouse & key).fetch1('name', 'doa', 'training_start')\n",
    "\n",
    "    # add subject information to NWB file\n",
    "    nwbfile.subject = Subject(\n",
    "        subject_id = str(mouse_id),\n",
    "        date_of_birth = datetime.combine(doa-timedelta(days=56), datetime.min.time()), # date of arrival minus 8 weeks\n",
    "        description = f'Mouse {mouse_id} {mouse_name}, date of arrival {doa} (~8 weeks old; date of birth is approximated), training started on {training_start}',\n",
    "        species = 'Mus musculus', \n",
    "        sex = 'M',\n",
    "        strain = 'C57BL/6J',\n",
    "        genotype = 'wildtype'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### ADD SYNC DATA ####\n",
    "    # create TimeSeries for signals in sync data\n",
    "    vel_raw = TimeSeries(\n",
    "        name = 'raw_velocity_timeseries',\n",
    "        data = sync_data[0:3,:].T,\n",
    "        unit = 'A.U.',\n",
    "        rate = sync_rate,\n",
    "        description = 'Raw velocity of the spherical treadmill sampled at 2000 Hz, array of the shape: timestampes x [pitch, roll, yaw]'\n",
    "    )\n",
    "\n",
    "    imaging_frame_raw = TimeSeries(\n",
    "        name = 'imaging_frame_timeseries',\n",
    "        data = sync_data[3,:],\n",
    "        unit = 'A.U.',\n",
    "        rate = sync_rate,\n",
    "        description = 'Two photon imaging frame signals sampled at 2000 Hz'\n",
    "    )\n",
    "\n",
    "    virmen_frame_raw = TimeSeries(\n",
    "        name = 'virmen_frame_timeseries',\n",
    "        data = sync_data[4,:],\n",
    "        unit = 'A.U.',\n",
    "        rate = sync_rate,\n",
    "        description = 'Virmen frame signals sampled at 2000 Hz'\n",
    "    )\n",
    "\n",
    "    lick_raw = TimeSeries(\n",
    "        name = 'lick_timeseries',\n",
    "        data = sync_data[5,:],\n",
    "        unit = 'A.U.',\n",
    "        rate = sync_rate,\n",
    "        description = 'Analog lick signals sampled at 2000 Hz (some sessions have signal loss due to acquisition problems, so not a reliable signal)'\n",
    "    )\n",
    "    \n",
    "    # add sync timeseries data to acquisition\n",
    "    nwbfile.add_acquisition(vel_raw)\n",
    "    nwbfile.add_acquisition(imaging_frame_raw)\n",
    "    nwbfile.add_acquisition(virmen_frame_raw)\n",
    "    nwbfile.add_acquisition(lick_raw)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### ADD VIRMEN ACQUISITION DATA ####\n",
    "    # fetch sync object\n",
    "    syncObj = (swac.CaSourceExtraction & key).fetch1('sources_sync_data')\n",
    "    sync_names = list(syncObj.dtype.names)\n",
    "    syncDict = {sync_names[ind]:syncObj[sync_names[ind]][0][0] for ind, item in enumerate(sync_names)}\n",
    "     \n",
    "    # get length for behavioral data and sync data of Virmen frames \n",
    "    behav_timeseries_length_verm = behav_data.shape[1]\n",
    "    behav_timeseries_length_sync = syncDict['vermMidTimes'].shape[0]\n",
    "\n",
    "    # find the shorter one\n",
    "    behav_timeseries_length = np.minimum(behav_timeseries_length_sync, behav_timeseries_length_verm)\n",
    "    \n",
    "    # create TimeSeries for signals in Virmen behavioral data\n",
    "    virmen_timestamps = ((syncDict['vermMidTimes'][:behav_timeseries_length])/sync_rate).flatten() # we have extra virmen frames from sync data than behavioral data\n",
    "\n",
    "\n",
    "    # create spatial series for frame aligned position\n",
    "    virmen_position_raw = SpatialSeries(\n",
    "        name = 'Virmen_forward_lateral_positions', \n",
    "        description = 'Raw forward and lateral position in Virmen frames (sampled at ~30 Hz)',\n",
    "        data = np.stack((for_pos[:]/100., lat_pos/100.),axis=1)[:behav_timeseries_length,:], # 1 Virmen unit ~-= 1 cm, convert from cm to m\n",
    "        timestamps = virmen_timestamps,\n",
    "        unit = 'm',\n",
    "        reference_frame = '(0, 0) is the start point (centered) of the maze stem'\n",
    "    )\n",
    "\n",
    "    # wrap the SpatialSeries in a Position object\n",
    "    position_obj_raw = Position(\n",
    "        name = 'Virmen_positions',\n",
    "        spatial_series = virmen_position_raw)\n",
    "\n",
    "\n",
    "    virmen_vel_raw = TimeSeries(\n",
    "        name = 'Virmen_forward_lateral_velocity_timeseries',\n",
    "        data = np.stack((for_vel_verm, lat_vel_verm),axis=1)[:behav_timeseries_length,:],\n",
    "        unit = 'A.U.',\n",
    "        timestamps = virmen_position_raw, \n",
    "        description = 'Raw forward and lateral velocity (in maze) in Virmen frames (sampled at ~30 Hz)'\n",
    "    )\n",
    "\n",
    "\n",
    "    trial_phase_raw = TimeSeries(\n",
    "        name = 'Virmen_trial_phase_timeseries',\n",
    "        data = trial_phase[:behav_timeseries_length],\n",
    "        unit = 'N.A.',\n",
    "        timestamps = virmen_position_raw, \n",
    "        description = 'Trial phase in Virmen frames (sampled at ~30 Hz), 0: maze, -1: feedback delay/feedback, -2: visual feedback onset, 1: ITI'\n",
    "    )\n",
    "\n",
    "    trial_num_raw = TimeSeries(\n",
    "        name = 'Virmen_trial_number_timeseries',\n",
    "        data = trial_num[:behav_timeseries_length],\n",
    "        unit = 'N.A.',\n",
    "        timestamps = virmen_position_raw, \n",
    "        description = 'Trial number in Virmen frames (sampled at ~30 Hz)'\n",
    "    )\n",
    "\n",
    "    reward_raw = TimeSeries(\n",
    "        name = 'Virmen_reward_timeseries',\n",
    "        data = reward[:behav_timeseries_length].astype(bool),\n",
    "        unit = 'N.A.',\n",
    "        timestamps = virmen_position_raw, \n",
    "        description = 'Reward in Virmen frames (sampled at ~30 Hz), 1: reward delivered'\n",
    "    )\n",
    "\n",
    "    world_raw = TimeSeries(\n",
    "        name = 'Virmen_maze_world_timeseries',\n",
    "        data = world[:behav_timeseries_length],\n",
    "        unit = 'N.A.',\n",
    "        timestamps = virmen_position_raw, \n",
    "        description = 'Maze world in Virmen frames (sampled at ~30 Hz), 1:WLV, 2:BRV, 3:WRV, 4:BLV, 5:WLN, 6:BRN, 7:WRN, 8:BLN; B/W:black/white cue, L/R:left/right reward location, V/N:visually-guided/normal trial'\n",
    "    ) \n",
    "    \n",
    "    # add Virmen timeseries data to acquisition\n",
    "    nwbfile.add_acquisition(position_obj_raw)\n",
    "    nwbfile.add_acquisition(virmen_vel_raw)\n",
    "    nwbfile.add_acquisition(trial_num_raw)\n",
    "    nwbfile.add_acquisition(trial_phase_raw)\n",
    "    nwbfile.add_acquisition(reward_raw)\n",
    "    nwbfile.add_acquisition(world_raw)\n",
    "    \n",
    "    #### ADD TRIAL DATA ####\n",
    "    # fetch framed aligned behavioral data\n",
    "    tI = (swac_beh.BehaviorStructure & key).fetch1('behav_struct')\n",
    "    trialVar = tI.dtype.names\n",
    "    trial_dict = {trialVar[ind]:tI[trialVar[ind]][0][0] for ind, item in enumerate(trialVar)}\n",
    "    nTrials = trial_dict['trialCueB'].shape[1]\n",
    "    \n",
    "    # fetch strategy variables (order for strat_pred: WR, WL, BR, BL)\n",
    "    h, mi, rule_belief, strat_preds = (swac_beh.BehaviorRNN & key).fetch1('strat_h','strat_mi','strat_rule','strat_preds')\n",
    "\n",
    "    # compute derived strategy variables\n",
    "    mpa, mpa_cho, mpa_cue = model_prob_of_action(strat_preds, trial_dict)\n",
    "    signed_bias = strat_preds[[1,3],:].sum(axis=0) - strat_preds[[0,2],:].sum(axis=0)\n",
    "    association_matrix = strat_preds*2 # multiply by two to obtain conditional probability values\n",
    "    \n",
    "    # find rule (context) for every trial\n",
    "    rule_A_world = [3,4,7,8] # BL,WR\n",
    "    rule_B_world = [1,2,5,6] # WL,BR\n",
    "    is_ruleA = np.array([True if this_world in rule_A_world else False for this_world in syncDict['tWorld']])\n",
    "    is_switch = np.array([True if this_trial in trial_dict['switches'] else False for this_trial in range(nTrials)])\n",
    "    \n",
    "    # add and define all the trial column\n",
    "    nwbfile.add_trial_column(name='is_vis', description='whether the trial was a visually guided trial (True: visually guided, False: non-visually guided)')\n",
    "    nwbfile.add_trial_column(name='is_ruleA', description='whether the trial happened during rule A (True: rule A (BL,WR), False: rule B(WL,BR))')\n",
    "    nwbfile.add_trial_column(name='is_switch', description='whether a rule switch happened on the trial (True: switch, False: non-switch)')\n",
    "\n",
    "    nwbfile.add_trial_column(name='is_cueB', description='whether the trial had a black cue (True: black, False: white)')\n",
    "    nwbfile.add_trial_column(name='is_choL', description='whether the mouse made a left choice on the trial (True: left, False: right)')\n",
    "    nwbfile.add_trial_column(name='is_correct', description='whether the trial was correct (True: correct, False: incorrect)')\n",
    "\n",
    "    # nwbfile.add_trial_column(name='prevCueB', description='whether the previous trial had a black cue (1: black, 0: white, or NaN for trial 0)')\n",
    "    # nwbfile.add_trial_column(name='prevChoL', description='whether the mouse made a left choice on the previous trial (1: left, 0: right, or NaN for trial 0)')\n",
    "    # nwbfile.add_trial_column(name='prevCorr', description='whether the previous trial was correct (1: correct, 0: incorrect, or NaN for trial 0)')\n",
    "\n",
    "    nwbfile.add_trial_column(name='association_mat', description='behavioral LSTM predicted conditional probability, order: P(R|W), P(L|W), P(R|B), P(L|B)')\n",
    "    nwbfile.add_trial_column(name='rule_belief', description='the rule belief on the trial (pos: rule B, neg: rule A)')\n",
    "    nwbfile.add_trial_column(name='signed_bias', description='the singed choice bias on the trial (pos: left bias, neg: right bias)')\n",
    "    nwbfile.add_trial_column(name='rule_following', description='the rule-following on the trial (between -0.5 ~ 0.5)')\n",
    "    nwbfile.add_trial_column(name='bias_following', description='the bias-following on the trial (between -0.5 ~ 0.5)')\n",
    "    nwbfile.add_trial_column(name='prob_actual_cho', description='the probability of actual choice on the trial, i.e. P(actual choice|actual cue)')\n",
    "\n",
    "    nwbfile.add_trial_column(name='trial_onset_plane_frame_idx', description='correspoding imaging frame index for this trial onset for each imaging plane')\n",
    "    nwbfile.add_trial_column(name='trial_offset_plane_frame_idx', description='correspoding imaging frame index for this trial offset for each imaging plane')\n",
    "    \n",
    "    # loop over trials and add data into trial table\n",
    "    for i_trial in range(nTrials):\n",
    "        # find vermin frame for trial onset and offset\n",
    "        start_time_verm_frame = int(syncDict['trialStarts'][i_trial][0])\n",
    "        if i_trial != nTrials-1:\n",
    "            stop_time_verm_frame = int(syncDict['trialStarts'][i_trial+1][0])-1\n",
    "        else: \n",
    "            stop_time_verm_frame = virmen_timestamps.shape[0]-1 # choose last virmen frame as trial offset for the last trial\n",
    "\n",
    "        # add individual trials\n",
    "        nwbfile.add_trial(\n",
    "            start_time = syncDict['vermMidTimes'][start_time_verm_frame][0]/sync_rate,\n",
    "            stop_time = syncDict['vermMidTimes'][stop_time_verm_frame][0]/sync_rate,\n",
    "            is_vis = bool(trial_dict['trialVis'].flatten()[i_trial]),\n",
    "            is_ruleA = is_ruleA[i_trial],\n",
    "            is_switch = is_switch[i_trial],\n",
    "            is_cueB = bool(trial_dict['trialCueB'].flatten()[i_trial]),\n",
    "            is_choL = bool(trial_dict['trialChoL'].flatten()[i_trial]),\n",
    "            is_correct = bool(trial_dict['trialRew'].flatten()[i_trial]),\n",
    "    #         prevCueB = trial_dict['prevTrialCueB'].flatten()[i_trial],\n",
    "    #         prevChoL = trial_dict['prevTrialChoL'].flatten()[i_trial],\n",
    "    #         prevCorr = trial_dict['prevTrialRew'].flatten()[i_trial],\n",
    "            association_mat = association_matrix[:,i_trial],\n",
    "            rule_belief = rule_belief.flatten()[i_trial],\n",
    "            signed_bias = signed_bias[i_trial],\n",
    "            rule_following = mpa_cue[i_trial],\n",
    "            bias_following = mpa_cho[i_trial]-0.5,\n",
    "            prob_actual_cho = mpa[i_trial],\n",
    "            trial_onset_plane_frame_idx = trial_dict['trialOnsets'][:,i_trial].astype(int),\n",
    "            trial_offset_plane_frame_idx = trial_dict['trialOffsets'][:,i_trial].astype(int)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #### ADD EPOCH DATA ####\n",
    "    # add epoch columns\n",
    "    nwbfile.add_epoch_column(name = 'epoch_type', description = 'type of epoch: maze, maze_stem, maze_arm, feedback_delay, feedback, iti, feedback_and_iti')\n",
    "    nwbfile.add_epoch_column(name = 'trial_id', description = 'trial number for this epoch')\n",
    "    \n",
    "    # loop over trials and add data into epoch table\n",
    "    for i_trial in range(nTrials):\n",
    "        # identify virmen frames for each epoch\n",
    "        trial_verm_frame_idx = trial_num == i_trial\n",
    "        maze_onset = np.where(np.logical_and(trial_phase == 0, trial_verm_frame_idx))[0][0]\n",
    "        stem_onset = maze_onset.copy()\n",
    "\n",
    "        # find all frames in feedback delay and feedback period\n",
    "        all_feedback_frames = np.where(np.logical_and(trial_phase == -1, trial_verm_frame_idx))[0]\n",
    "\n",
    "        # if feedback frames exist\n",
    "        if all_feedback_frames.shape[0]>0:\n",
    "            feedback_delay_exist = True\n",
    "            feedback_delay_onset = all_feedback_frames[0]\n",
    "            feedback_iti_onset = feedback_delay_onset.copy()\n",
    "            maze_offset = feedback_delay_onset-1\n",
    "            funnel_offset = maze_offset.copy()\n",
    "        else:    \n",
    "            feedback_delay_exist = False\n",
    "            maze_offset = np.where(np.logical_and(trial_phase == 0, trial_verm_frame_idx))[0][-1]\n",
    "\n",
    "        # find all positions in the maze\n",
    "        these_pos = for_pos[maze_onset:maze_offset+1]\n",
    "        if these_pos[-1] > floorLength:\n",
    "            funnel_exist = True\n",
    "            stem_offset = np.where(np.logical_and(for_pos <= floorLength, trial_verm_frame_idx))[0][-1]\n",
    "            funnel_onset = stem_offset + 1\n",
    "        else:\n",
    "            funnel_exist = False\n",
    "            stem_offset = maze_offset.copy()\n",
    "\n",
    "        if feedback_delay_exist: # if feedback delay/feedback exist\n",
    "            feedback_onset = np.where(np.logical_and(trial_phase == -2, trial_verm_frame_idx))[0]\n",
    "            feedback_offset = np.where(trial_num == i_trial)[0][-1]\n",
    "\n",
    "            # if feedback onset frame doesn't exist\n",
    "            if feedback_onset.shape[0] == 0:\n",
    "                feedback_exist = False\n",
    "                feedback_delay_offset = all_feedback_frames[-1] # feedback delay offset is the last frame in all_feedback_frames\n",
    "                feedback_iti_offset = feedback_delay_offset.copy()\n",
    "            elif feedback_onset.shape[0] > 0:\n",
    "                feedback_exist = True\n",
    "                feedback_delay_offset = feedback_onset[0]-1\n",
    "\n",
    "                if i_trial < nTrials-1: # last trial does not have ITI\n",
    "                    iti_frames = np.where(np.logical_and(trial_phase == 1,trial_num == (i_trial + 1)))[0] # this trial's ITI is labeled as next trial in trial_num\n",
    "                    feedback_iti_offset = iti_frames[-1]\n",
    "                else:\n",
    "                    feedback_iti_offset = feedback_offset.copy()\n",
    "\n",
    "        # add epochs    \n",
    "        nwbfile.add_epoch(\n",
    "            start_time = syncDict['vermMidTimes'].flatten()[maze_onset]/sync_rate,\n",
    "            stop_time = syncDict['vermMidTimes'].flatten()[maze_offset]/sync_rate,\n",
    "            trial_id = i_trial,\n",
    "            epoch_type = 'maze'\n",
    "        )\n",
    "\n",
    "        nwbfile.add_epoch(\n",
    "            start_time = syncDict['vermMidTimes'].flatten()[stem_onset]/sync_rate,\n",
    "            stop_time = syncDict['vermMidTimes'].flatten()[stem_offset]/sync_rate,\n",
    "            trial_id = i_trial,\n",
    "            epoch_type = 'maze_stem'\n",
    "        )\n",
    "\n",
    "        if funnel_exist:\n",
    "            nwbfile.add_epoch(\n",
    "                start_time = syncDict['vermMidTimes'].flatten()[funnel_onset]/sync_rate,\n",
    "                stop_time = syncDict['vermMidTimes'].flatten()[funnel_offset]/sync_rate,\n",
    "                trial_id = i_trial,\n",
    "                epoch_type = 'maze_arm'\n",
    "            )\n",
    "\n",
    "        if feedback_delay_exist:\n",
    "            nwbfile.add_epoch(\n",
    "                start_time = syncDict['vermMidTimes'].flatten()[feedback_delay_onset]/sync_rate,\n",
    "                stop_time = syncDict['vermMidTimes'].flatten()[feedback_delay_offset]/sync_rate,\n",
    "                trial_id = i_trial,\n",
    "                epoch_type = 'feedback_delay'\n",
    "            )\n",
    "            if feedback_exist:\n",
    "                nwbfile.add_epoch(\n",
    "                    start_time = syncDict['vermMidTimes'].flatten()[feedback_onset[0]]/sync_rate,\n",
    "                    stop_time = syncDict['vermMidTimes'].flatten()[all_feedback_frames[-1]]/sync_rate,\n",
    "                    trial_id = i_trial,\n",
    "                    epoch_type = 'feedback'\n",
    "                )\n",
    "\n",
    "\n",
    "        if i_trial < nTrials-1: # last trial does not have ITI\n",
    "            nwbfile.add_epoch(\n",
    "                start_time = syncDict['vermMidTimes'].flatten()[iti_frames[0]]/sync_rate,\n",
    "                stop_time = syncDict['vermMidTimes'].flatten()[iti_frames[-1]]/sync_rate,\n",
    "                trial_id = i_trial,\n",
    "                epoch_type = 'iti'\n",
    "            )\n",
    "\n",
    "        if feedback_delay_exist:\n",
    "            nwbfile.add_epoch(\n",
    "                start_time = syncDict['vermMidTimes'].flatten()[feedback_iti_onset]/sync_rate,\n",
    "                stop_time = syncDict['vermMidTimes'].flatten()[feedback_iti_offset]/sync_rate,\n",
    "                trial_id = i_trial,\n",
    "                epoch_type = 'feedback_and_iti'\n",
    "            )\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### ADD PROCESSED BEHAVIORAL DATA ####\n",
    "    # create processing module for processed behavioral data\n",
    "    behavior_module = nwbfile.create_processing_module(\n",
    "        name = 'behavior', \n",
    "        description = 'processed behavioral data'\n",
    "    )\n",
    "\n",
    "    # reshape frameMidTimes\n",
    "    if img_type == 'volume':\n",
    "        nPlanes = 5\n",
    "        nImgPlanes = 4\n",
    "        frameMidTimes_reshape = syncDict['frameMidTimes'].reshape(-1,nPlanes)\n",
    "    elif img_type == 'plane':\n",
    "        nPlanes = 1\n",
    "        nImgPlanes = 1\n",
    "        # create 6 Hz frames (velRNN is modeled at 6 Hz) \n",
    "        nFrames_vol = int(np.floor(syncDict['frameMidTimes'].shape[0]/5))\n",
    "        frameMidTimes_reshape = syncDict['frameMidTimes'][:nFrames_vol*5,:].reshape(-1,5)\n",
    "\n",
    "    # fetch frame aligned velocity\n",
    "    frame_pitch, frame_roll, frame_yaw = (swac.FrameAlignedVel & key).fetch1('frame_pitch', 'frame_roll', 'frame_yaw')\n",
    "\n",
    "    # fetch velRNN values\n",
    "    valid_frames_rnn, cho_f_score_rnn, cho_r_score_rnn, cue_f_score_rnn, cue_r_score_rnn = (swac_beh.VelPredRNN & key).fetch1(\n",
    "        'valid_frames_rnn','cho_f_score_rnn','cho_r_score_rnn','cue_f_score_rnn','cue_r_score_rnn')\n",
    "\n",
    "    valid_frames_rnn = valid_frames_rnn.reshape(-1,)\n",
    "\n",
    "    cho_pred_f = np.full(valid_frames_rnn.shape, np.NaN)\n",
    "    cho_pred_r = np.full(valid_frames_rnn.shape, np.NaN)\n",
    "    cue_pred_f = np.full(valid_frames_rnn.shape, np.NaN)\n",
    "    cue_pred_r = np.full(valid_frames_rnn.shape, np.NaN)\n",
    "\n",
    "    cho_pred_f[valid_frames_rnn] = cho_f_score_rnn.reshape(-1,)\n",
    "    cho_pred_r[valid_frames_rnn] = cho_r_score_rnn.reshape(-1,)\n",
    "    cue_pred_f[valid_frames_rnn] = cue_f_score_rnn.reshape(-1,)\n",
    "    cue_pred_r[valid_frames_rnn] = cue_r_score_rnn.reshape(-1,)\n",
    "    \n",
    "    # unfold framed aligned signals\n",
    "    posL_unfold = trial_dict['posL'].T.reshape(-1,)\n",
    "    posF_unfold = trial_dict['posF'].T.reshape(-1,)\n",
    "    frameTrialMem_unfold = trial_dict['frameTrialMem'].T.reshape(-1,) - 1 # convert first trial to 0\n",
    "    choFrameOffsets_unfold = trial_dict['choFrameOffsets'].T.reshape(-1,)\n",
    "    pitch_unfold = frame_pitch.T.reshape(-1,)\n",
    "    roll_unfold = frame_roll.T.reshape(-1,)\n",
    "    yaw_unfold = frame_yaw.T.reshape(-1,)\n",
    "    \n",
    "    # create spatial series for frame aligned position\n",
    "    position_spatial_series = SpatialSeries(\n",
    "        name = 'frame_aligned_forward_and_lateral_position', \n",
    "        description = 'forward and lateral position aligned to imaging frames (30 Hz)',\n",
    "        data = np.stack((posF_unfold/100., posL_unfold/100.), axis = 1), # convert from cm to m \n",
    "        unit = 'm',\n",
    "        timestamps = syncDict['frameMidTimes'].flatten()/sync_rate,\n",
    "        reference_frame = '(0, 0) is the start point (centered) of the maze stem'\n",
    "    )\n",
    "\n",
    "    # wrap the SpatialSeries in a Position object\n",
    "    position_obj = Position(\n",
    "        name = 'frame_aligned_position',\n",
    "        spatial_series = position_spatial_series)\n",
    "\n",
    "    # add Position into behavioral processing module\n",
    "    behavior_module.add(position_obj)\n",
    "    \n",
    "    # create time series for frame aligned velocity\n",
    "    velocity_time_series = TimeSeries(\n",
    "        name = 'frame_aligned_pitch_roll_yaw_velocity',\n",
    "        data = np.stack((pitch_unfold, roll_unfold, yaw_unfold), axis = 1), \n",
    "        timestamps = position_spatial_series, # reusing the timestamps (frameMidTimes) stored in position_spatial_series\n",
    "        description = 'velocity of the spherical treadmill (pitch, roll, yaw) aligned to imaging frames (30 Hz)',\n",
    "        unit = 'A.U.'\n",
    "    )\n",
    "\n",
    "    # wrap the TimeSeries in a BehavioralTimeSeries object\n",
    "    behavioral_time_series_vel = BehavioralTimeSeries(\n",
    "        time_series = velocity_time_series,\n",
    "        name = 'frame_aligned_velocity'\n",
    "    )\n",
    "\n",
    "    # add the BehavioralTimeSeries into behavioral processing module\n",
    "    behavior_module.add(behavioral_time_series_vel)\n",
    "    \n",
    "    # create time series for velRNN\n",
    "    rnn_time_series = TimeSeries(\n",
    "        name = 'velocity_RNN_prediction_for_choice_and_cue',\n",
    "        data = np.stack((cho_pred_f, cho_pred_r, cue_pred_f, cue_pred_r),axis=1), \n",
    "        timestamps = frameMidTimes_reshape[:,2]/sync_rate, # take the mid slice \n",
    "        unit = 'N.A.',\n",
    "        description = 'velocity RNN prediction (forward choice, reverse choice, forward cue, reverse cue) aligned to mid imaging frames (idx=2) of each 6 Hz volume (or pseudovolume for plane imaging)',\n",
    "    )\n",
    "\n",
    "    # wrap the TimeSeries in a BehavioralTimeSeries object\n",
    "    behavioral_time_series_rnn = BehavioralTimeSeries(\n",
    "        time_series = rnn_time_series,\n",
    "        name = 'velocity_RNN_prediction_for_choice_and_cue'\n",
    "    )\n",
    "\n",
    "    # add the BehavioralTimeSeries into behavioral processing module\n",
    "    behavior_module.add(behavioral_time_series_rnn)\n",
    "    \n",
    "    # create time series for frameTrialMem\n",
    "    frameTrialMem_time_series = TimeSeries(\n",
    "        name = 'frame_aligned_trial_number',\n",
    "        data = frameTrialMem_unfold, \n",
    "        timestamps = position_spatial_series, # reusing the timestamps (frameMidTimes) stored in position_spatial_series\n",
    "        unit = 'N.A.',\n",
    "        description = 'trial number aligned to imaging frames',\n",
    "    )\n",
    "\n",
    "\n",
    "    # create time series for choFrameOffsets\n",
    "    choFrameOffsets_time_series = TimeSeries(\n",
    "        name = 'frame_aligned_time_from_choice_point',\n",
    "        data = choFrameOffsets_unfold, \n",
    "        timestamps = position_spatial_series,\n",
    "        unit = 's',\n",
    "        description = 'time elapsed (sec) from choice point aligned to imaging frames (neg: maze, pos: feedback and ITI)',\n",
    "    )\n",
    "\n",
    "    # create time series for ver2frame\n",
    "    verm2frame_time_series = TimeSeries(\n",
    "        name = 'verm_to_frame_index_conversion',\n",
    "        data = syncDict['verm2frame'].flatten()-1, # minus one to convert from matlab index to python index\n",
    "        timestamps = syncDict['vermMidTimes'].flatten()/sync_rate,\n",
    "        unit = 'N.A.',\n",
    "        description = 'correspoding imaging frame index for each verm frame'\n",
    "    )\n",
    "\n",
    "    # create time series for frame2verm\n",
    "    frame2verm_time_series = TimeSeries(\n",
    "        name = 'frame_to_verm_index_conversion',\n",
    "        data = syncDict['frame2verm'].flatten()-1,  # minus one to convert from matlab index to python index\n",
    "        timestamps = position_spatial_series, \n",
    "        unit = 'N.A.',\n",
    "        description = 'correspoding verm frame index for each imaging frame'\n",
    "    )\n",
    "\n",
    "    # add these TimeSeries into behavioral processing module\n",
    "    behavior_module.add(frameTrialMem_time_series)\n",
    "    behavior_module.add(choFrameOffsets_time_series)\n",
    "    behavior_module.add(verm2frame_time_series)\n",
    "    behavior_module.add(frame2verm_time_series)\n",
    "\n",
    "    # create plane number for imaging frames\n",
    "    if img_type == 'volume':\n",
    "        nFrames_per_plane = cho_pred_f.shape[0]\n",
    "        plane_idx = repmat(np.array([0,1,2,3,4]), 1, nFrames_per_plane).reshape(-1,)\n",
    "\n",
    "        # create time series for plane_num\n",
    "        plane_idx_time_series = TimeSeries(\n",
    "            name = 'plane_idx_for_imaging_frames',\n",
    "            data = plane_idx, \n",
    "            timestamps = position_spatial_series,\n",
    "            unit = 'N.A.',\n",
    "            description = 'plane index for imaging frames'\n",
    "        )\n",
    "\n",
    "        # add the TimeSeries into behavioral processing module\n",
    "        behavior_module.add(plane_idx_time_series)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #### ADD 2P IMAGING DATA ####\n",
    "    # create processing modules for 2p imaging \n",
    "    ophys_module = nwbfile.create_processing_module(\n",
    "        name = 'ophys', \n",
    "        description = 'two photon calcium imaging processed data'\n",
    "    )\n",
    "\n",
    "    # create ImageSegmentation object\n",
    "    img_seg = ImageSegmentation()\n",
    "    \n",
    "    # fetch FOV center location in CCF\n",
    "    tmat, center_ml_ccf_mm, center_ap_ccf_mm, depth_mm = (swac.FOVtoCCF & key).fetch1('tmat_fov_to_ccf_mm','center_ml_ccf_mm', 'center_ap_ccf_mm', 'depth_mm')\n",
    "\n",
    "    # compute pixel size and orgin location\n",
    "    fov_ap_size = 675./1000\n",
    "    fov_ml_size = 750./1000\n",
    "    mm_per_pixel_ap = fov_ap_size/512\n",
    "    mm_per_pixel_ml = fov_ml_size/512\n",
    "    # FOV direction: x-axis: posterior to anterior; y-axis: laterl (left) to medial (right)\n",
    "    origin_x = np.matmul(tmat, np.array([[1,1,1]]).T).reshape(-1,)[1] # ap # the origin is [1,1,1] because it was the first pixel in Matlab indexing\n",
    "    origin_y = np.matmul(tmat, np.array([[1,1,1]]).T).reshape(-1,)[0] # ml\n",
    "\n",
    "    # create Device, OptionalChannel and ImagingPlane\n",
    "    device = nwbfile.create_device(\n",
    "        name = 'two_photon_microscope', \n",
    "        description = 'Two-photon microscope',\n",
    "        manufacturer = 'Harvey Lab custom-bulit two-photon microscope (Odin)'\n",
    "    )\n",
    "\n",
    "    # create OpticalChannel for GCaMP\n",
    "    optical_channel = OpticalChannel(\n",
    "        name = 'OpticalChannel_GCaMP', \n",
    "        description = 'optical channel for GCaMP', \n",
    "        emission_lambda = 525.\n",
    "    )\n",
    "\n",
    "    # create ImagingPlane for GCaMP\n",
    "    imaging_plane = nwbfile.create_imaging_plane(\n",
    "        name = 'ImagingPlane_GCaMP6s_920nm',\n",
    "        optical_channel = optical_channel,\n",
    "        imaging_rate = 30.,\n",
    "        description = f'{img_type} calcium imaging in {fov_center_area} layer {layer}',\n",
    "        device = device,\n",
    "        excitation_lambda = 920.,\n",
    "        indicator = 'GCaMP6s',\n",
    "        location = fov_center_area,\n",
    "        grid_spacing = [mm_per_pixel_ap, mm_per_pixel_ml, step_size/1000.],\n",
    "        grid_spacing_unit = 'mm',\n",
    "        origin_coords = [origin_x, origin_y, depth_mm],\n",
    "        origin_coords_unit = 'mm',\n",
    "        reference_frame = f'[p->a, l->m, depth from pia] in minimeters from bregma in Allen Institute Mouse Common Coordinate Framework (CCF); i.e. directionality of the imaging FOV: x-axis: posterior to anterior; y-axis: laterl (left) to medial (right); however, the FOV axes does not run parallelly to the CCF coordinate system. The following transformation matrix can be applied to a column vector of pixel coordiate ([x,y,1].T) to [ml, ap, _] in mm in CCF: np.array([[{tmat[0,0]:.6f},{tmat[0,1]:.6f},{tmat[0,2]:.6f}],[{tmat[1,0]:.6f},{tmat[1,1]:.6f},{tmat[1,2]:.6f}],[{tmat[2,0]:.6f},{tmat[2,1]:.6f},{tmat[2,2]:.6f}]])'\n",
    "    )\n",
    "\n",
    "    # create OpticalChannel for mTagBFP2\n",
    "    optical_channel_mTagBFP2 = OpticalChannel(\n",
    "        name = 'OpticalChannel_mTagBFP2', \n",
    "        description = 'optical channel for mTagBFP2', \n",
    "        emission_lambda = 450.\n",
    "    )\n",
    "\n",
    "    # create ImagingPlane for mTagBFP2\n",
    "    imaging_plane_mTagBFP2 = nwbfile.create_imaging_plane(\n",
    "        name = 'ImagingPlane_mTagBFP2_850nm',\n",
    "        optical_channel = [optical_channel_mTagBFP2, optical_channel],\n",
    "        imaging_rate = 30.,\n",
    "        description = 'mTagBFP2/GCaMP6s imaging for AAVretro labeling',\n",
    "        device = device,\n",
    "        excitation_lambda = 850.,\n",
    "        indicator = 'mTagBFP2',\n",
    "        location = fov_center_area,\n",
    "        grid_spacing = [mm_per_pixel_ap, mm_per_pixel_ml, step_size/1000.],\n",
    "        grid_spacing_unit = 'mm',\n",
    "        origin_coords = [origin_x, origin_y, depth_mm],\n",
    "        origin_coords_unit = 'mm',\n",
    "        reference_frame = f'[p->a, l->m, depth from pia] in minimeters from bregma in Allen Institute Mouse Common Coordinate Framework (CCF); i.e. directionality of the imaging FOV: x-axis: posterior to anterior; y-axis: laterl (left) to medial (right); however, the FOV axes does not run parallelly to the CCF coordinate system. The following transformation matrix can be applied to a column vector of pixel coordiate ([x,y,1].T) to [ml, ap, _] in mm in CCF: np.array([[{tmat[0,0]:.6f},{tmat[0,1]:.6f},{tmat[0,2]:.6f}],[{tmat[1,0]:.6f},{tmat[1,1]:.6f},{tmat[1,2]:.6f}],[{tmat[2,0]:.6f},{tmat[2,1]:.6f},{tmat[2,2]:.6f}]])'\n",
    "    )\n",
    "\n",
    "    # create OpticalChannel for mScarlet\n",
    "    optical_channel_mScarlet = OpticalChannel(\n",
    "        name = 'OpticalChannel_mScarlet', \n",
    "        description = 'optical channel for mScarlet', \n",
    "        emission_lambda = 625.\n",
    "    )\n",
    "\n",
    "    # create ImagingPlane for mScarlet\n",
    "    imaging_plane_mScarlet = nwbfile.create_imaging_plane(\n",
    "        name = 'ImagingPlane_mScarlet_800nm',\n",
    "        optical_channel = [optical_channel_mScarlet, optical_channel],\n",
    "        imaging_rate = 30.,\n",
    "        description = 'mScarlet/GCaMP6s imaging for AAVretro labeling',\n",
    "        device = device,\n",
    "        excitation_lambda = 800.,\n",
    "        indicator = 'mScarlet',\n",
    "        location = fov_center_area,\n",
    "        grid_spacing = [mm_per_pixel_ap, mm_per_pixel_ml, step_size/1000.],\n",
    "        grid_spacing_unit = 'mm',\n",
    "        origin_coords = [origin_x, origin_y, depth_mm],\n",
    "        origin_coords_unit = 'mm',\n",
    "        reference_frame = f'[p->a, l->m, depth from pia] in minimeters from bregma in Allen Institute Mouse Common Coordinate Framework (CCF); i.e. directionality of the imaging FOV: x-axis: posterior to anterior; y-axis: laterl (left) to medial (right); however, the FOV axes does not run parallelly to the CCF coordinate system. The following transformation matrix can be applied to a column vector of pixel coordiate ([x,y,1].T) to [ml, ap, _] in mm in CCF: np.array([[{tmat[0,0]:.6f},{tmat[0,1]:.6f},{tmat[0,2]:.6f}],[{tmat[1,0]:.6f},{tmat[1,1]:.6f},{tmat[1,2]:.6f}],[{tmat[2,0]:.6f},{tmat[2,1]:.6f},{tmat[2,2]:.6f}]])'\n",
    "    )\n",
    "    \n",
    "    # fetch vessel pattern image\n",
    "    vessel_img = (swac.FOVinitialization & key).fetch1('overview')\n",
    "    \n",
    "    if vessel_img is not None:\n",
    "        # create objects for vessel pattern image\n",
    "        vessel = GrayscaleImage(\n",
    "            name = 'vessel_img',\n",
    "            data = vessel_img,\n",
    "            description = 'image of vessel pattern at brain surface for this FOV'\n",
    "        )\n",
    "        # put image in an Images container\n",
    "        vessel_image = Images(\n",
    "            name = 'vessel_img',\n",
    "            images = [vessel],\n",
    "            description = 'image of vessel pattern at brain surface for this FOV',\n",
    "        )\n",
    "        # add to ophys module\n",
    "        ophys_module.add(vessel_image)\n",
    "        \n",
    "    else: print('Vessel image does not exist!')\n",
    "        \n",
    "    # fetch gcamp and retrograde images \n",
    "    isRetrolabel = len(swac.RetrolabelUnmixing & key) > 0\n",
    "    if isRetrolabel:\n",
    "        blue_img, red_img, green850_img, green800_img = (swac.RetrolabelUnmixing & key).fetch1('blue', 'red', 'green850', 'green800')\n",
    "        if img_type == 'plane':\n",
    "            blue_img = blue_img[:,:,np.newaxis]\n",
    "            red_img = red_img[:,:,np.newaxis]\n",
    "            green850_img = green850_img[:,:,np.newaxis]\n",
    "            green800_img = green800_img[:,:,np.newaxis]\n",
    "\n",
    "        for nPlane in range(nImgPlanes):\n",
    "\n",
    "            # create objects for gcamp and retrograde label images\n",
    "            gcamp_img_850 = GrayscaleImage(\n",
    "                name = 'GCaMP_850nm',\n",
    "                data = green850_img[:,:,nPlane],\n",
    "                description = 'static unmixed GCaMP image of the FOV with excitation 850 nm'\n",
    "            )\n",
    "            gcamp_img_800 = GrayscaleImage(\n",
    "                name = 'GCaMP_800nm',\n",
    "                data = green800_img[:,:,nPlane],\n",
    "                description = 'static unmixed GCaMP image of the FOV with excitation 800 nm'\n",
    "            )\n",
    "            mTagBFP2_img = GrayscaleImage(\n",
    "                name = 'mTagBFP2_850nm',\n",
    "                data = blue_img[:,:,nPlane],\n",
    "                description = 'static unmixed mTagBFP2 image of the FOV with excitation 850 nm'\n",
    "            )\n",
    "            mScarlet_img = GrayscaleImage(\n",
    "                name = 'mScarlet_800nm',\n",
    "                data = red_img[:,:,nPlane],\n",
    "                description = 'static unmixed mScarlet image of the FOV with excitation 800 nm'\n",
    "            )\n",
    "\n",
    "            # put image in an Images container\n",
    "            static_images = Images(\n",
    "                name = f'static_GCaMP_and_retrograde_label_image_plane_{nPlane}',\n",
    "                images = [gcamp_img_850, gcamp_img_800, mTagBFP2_img, mScarlet_img],\n",
    "                description = \"static unmixed images for GCaMP and retrograde labels of the FOV\",\n",
    "            )\n",
    "\n",
    "            ophys_module.add(static_images)\n",
    "            \n",
    "    else: print('Retrolabel images does not exist!')\n",
    "            \n",
    "            \n",
    "    # fetch retrolabel threshold\n",
    "    thresh_l23b, thresh_l23r, thresh_l5r = (swac.RetrolabelThresh & 'thresh_type=\"M\"').fetch1('thresh_l23b', 'thresh_l23r', 'thresh_l5r')\n",
    "\n",
    "    # create area-number mapping\n",
    "    area_name_dict = {0:'V1',1:'PM',2:'AM',3:'MM',4:'RSC',5:'A'}\n",
    "\n",
    "    # loop over imaging planes\n",
    "    for nPlane in range(nImgPlanes):\n",
    "\n",
    "        # create PlaneSegmentation for this slice\n",
    "        ps = img_seg.create_plane_segmentation(\n",
    "            name = 'PlaneSegmentation_' + str(nPlane),\n",
    "            description = f'Plane segmentation of imagine plane {nPlane}',\n",
    "            imaging_plane = imaging_plane  \n",
    "        )\n",
    "\n",
    "        # add extra fields for source location and retrograde labeling\n",
    "        ps.add_column(name = 'ml', description = 'ml location (mm from bregma)')\n",
    "        ps.add_column(name = 'ap', description = 'ap location (mm from bregma)')\n",
    "        ps.add_column(name = 'depth', description = 'depth (mm from pia)')\n",
    "        ps.add_column(name = 'area', description = 'cortical area where this cell is located')\n",
    "\n",
    "        # add column for retrolabel\n",
    "        if isRetrolabel:\n",
    "            ps.add_column(name = 'mTagBFP2', description = 'whether this cell is labeled with mTagBFP2')\n",
    "            ps.add_column(name = 'mScarlet', description = 'whether this cell is labeled with Scarlet')\n",
    "\n",
    "        # fetch source info and activity\n",
    "        source_ind, source_filter, source_ml, source_ap, source_depth, source_df, source_deconv, source_g, source_lam = (swac.SourceLocation*swac.SourceTrace*swac.Source & key & 'source_type=\"cell\"'& f'source_slice={nPlane+1}').fetch(\n",
    "            'source_ind', 'source_filter', 'source_ml', 'source_ap', 'source_depth', 'source_df', 'source_deconv', 'source_g', 'source_lam', order_by='source_ind')\n",
    "\n",
    "        # fetch retrograde label\n",
    "        if isRetrolabel:\n",
    "            bscore, rscore = (swac.Retrolabel*swac.Source & key & 'source_type=\"cell\"'& f'source_slice={nPlane+1}').fetch('bscore', 'rscore', order_by='source_ind')\n",
    "            if layer == '2/3':\n",
    "                is_blue = bscore >= thresh_l23b\n",
    "                is_red = rscore >= thresh_l23r\n",
    "            elif layer == '5':\n",
    "                is_red = rscore >= thresh_l5r\n",
    "                is_blue = np.zeros_like(is_red, dtype=bool) # for layer 5, put all blue to False\n",
    "\n",
    "        # classify source into area\n",
    "        source_area_ind = partition_areas(source_ml, source_ap)\n",
    "        source_area = source_area_ind['PM']+2*source_area_ind['AM']+3*source_area_ind['MM']+4*source_area_ind['RSC']+5*source_area_ind['A/RL']\n",
    "        source_area_name = [area_name_dict[this_area] for this_area in source_area]\n",
    "\n",
    "        # add roi into PlaneSegmentation\n",
    "        n_sources = source_ind.shape[0]\n",
    "        for i_source in range(n_sources):\n",
    "            if isRetrolabel:\n",
    "                ps.add_roi(id = i_source, pixel_mask = source_filter[i_source], \n",
    "                           ml = source_ml[i_source], ap = source_ap[i_source], depth = source_depth[i_source], \n",
    "                           area = source_area_name[i_source], mTagBFP2 = is_blue[i_source], mScarlet = is_red[i_source])\n",
    "            else:\n",
    "                ps.add_roi(id = i_source, pixel_mask = source_filter[i_source], \n",
    "                           ml = source_ml[i_source], ap = source_ap[i_source], depth = source_depth[i_source], \n",
    "                           area = source_area_name[i_source])\n",
    "\n",
    "\n",
    "        # create ROI table region \n",
    "        rt_region = ps.create_roi_table_region(\n",
    "            region = [i_source for i_source in range(n_sources)],\n",
    "            description = f'the ROIs in PlaneSegmentation_{nPlane}'\n",
    "        )\n",
    "\n",
    "        if img_type == 'plane':\n",
    "            imaging_timestamps = syncDict['frameMidTimes'].flatten()\n",
    "        elif img_type == 'volume':\n",
    "            imaging_timestamps = frameMidTimes_reshape[:,nPlane]/sync_rate\n",
    "\n",
    "        # create RoiResponseSeries for deconvolved activity and df\n",
    "        deconv_activity = RoiResponseSeries(\n",
    "            name = f'deconvolved_activity_plane_{nPlane}',\n",
    "            description = f'Deconvolved activity for ROIs in PlaneSegmentation_{nPlane}',\n",
    "            data = np.concatenate(source_deconv, axis=0).T, \n",
    "            rois = rt_region,\n",
    "            unit = 'A.U.',\n",
    "            timestamps = imaging_timestamps,\n",
    "        )\n",
    "\n",
    "        df = RoiResponseSeries(\n",
    "            name = f'dF_over_F_plane_{nPlane}',\n",
    "            description = f'dF/F for ROIs in PlaneSegmentation_{nPlane}',\n",
    "            data = np.concatenate(source_df, axis=0).T, \n",
    "            rois = rt_region,\n",
    "            unit = 'A.U.',\n",
    "            timestamps = deconv_activity, # reusing the timestamps stored in deconv_activity\n",
    "        )\n",
    "\n",
    "        # put RoiResponseSeries for df into DfOverF\n",
    "        df_over_f = DfOverF(name = f'df_over_f_plane_{nPlane}', roi_response_series = df)\n",
    "\n",
    "        ophys_module.add(df_over_f)\n",
    "        ophys_module.add(deconv_activity)\n",
    "\n",
    "    # add ImageSegmentation to the ophys module\n",
    "    ophys_module.add(img_seg)\n",
    "    \n",
    "    \n",
    "    #### ADD LABMETADATA ####\n",
    "    if is_ruleA[0]:\n",
    "        initial_rule = 'A'\n",
    "    else:\n",
    "        initial_rule = 'B'\n",
    "\n",
    "    if img_type == 'volume':\n",
    "        imaging_type = 'multi-plane'\n",
    "    else:\n",
    "        imaging_type = 'single-plane'\n",
    "        \n",
    "    lab_meta_data = LabMetaDataSession(\n",
    "        TaskParam__maze_stem_length=floorLength,\n",
    "        TaskParam__maze_stem_width=floorWidth,\n",
    "        TaskParam__maze_arm_length=funnelLength,\n",
    "        TaskParam__maze_arm_width=funnelWidth,\n",
    "        TaskParam__cue_delay_length=float(delay),\n",
    "        TaskParam__wall_height=wallHeight,\n",
    "        TaskParam__max_position=hideCuePast,\n",
    "        TaskParam__meter_per_virmen_unit=0.01,\n",
    "        TaskParam__frac_non_visually_guided_trials=frac_no_checker,\n",
    "        TaskParam__choice_bias_penalty=float(penalty),\n",
    "        TaskParam__feedback_delay_sec=feedbackDelay,\n",
    "        TaskParam__reward_delay_sec=rewardDelay,\n",
    "        TaskParam__iti_correct_sec=itiCorrect,\n",
    "        TaskParam__iti_incorrect_sec=itiMissBase,\n",
    "        TaskParam__switches=trial_dict['switches'].flatten().astype(int),\n",
    "        TaskParam__initial_rule=initial_rule,\n",
    "        AAVretroInjSite__mTagBFP2=retro_site[mouse_id]['mTagBFP2'],\n",
    "        AAVretroInjSite__mScarlet=retro_site[mouse_id]['mScarlet'],\n",
    "        Imaging__imaging_type=imaging_type,\n",
    "        Imaging__num_of_frame_per_volume=nPlanes,\n",
    "        Imaging__num_of_imaging_plane=nImgPlanes,\n",
    "        Imaging__cortical_layer=layer,\n",
    "        Registration__fov_center_area=fov_center_area,\n",
    "        Registration__fov_center_ml_ccf_mm=center_ml_ccf_mm,\n",
    "        Registration__fov_center_ap_ccf_mm=center_ap_ccf_mm, \n",
    "        Registration__fov_depth_mm=depth_mm,\n",
    "        Registration__fov_plane_step_size_mm=step_size/1000., # convert from um to mm\n",
    "        Registration__fov_to_ccf_transformation_matrix=tmat\n",
    "    )\n",
    "    nwbfile.add_lab_meta_data(lab_meta_data=lab_meta_data)\n",
    "    \n",
    "    \n",
    "    #### SAVE NWB FILE ####\n",
    "    filePath = 'Z:\\\\HarveyLab\\\\Tier1\\\\Shih_Yi\\\\NWBfile\\\\'+ f'{mouse_id}\\\\mouse_{mouse_id}_{fov_center_area}_L{layer_name}_{img_type}_imaging_{session_date}.nwb'\n",
    "    with NWBHDF5IO(filePath, 'w') as io:\n",
    "        io.write(nwbfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
